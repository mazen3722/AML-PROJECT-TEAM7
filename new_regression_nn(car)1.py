# -*- coding: utf-8 -*-
"""New Regression NN(Car).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r0gLnuvHE4vZ6BMuAFG0ItzClibGYZQo
"""
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder, minmax_scale
import math

#from google.colab import drive
#drive.mount('/content/drive')

#!ls /content/drive/MyDrive/

car = pd.read_csv("vehicles.csv")

car.head()

#cheacking the number of rows and columns
car.shape

#Getting some Information about the dataset
car.info()

#Cheacking the numbur of missing values
car.isnull().sum()

# Remove column - 'url' since the details present in url is already available in
# columns - 'region', 'county', 'state'
car.drop(['id', 'url', 'region_url', 'VIN', 'image_url', 'description', 'county', 'lat', 'long', 'posting_date', 'size', 'state'], axis=1, inplace=True)

car.isna().sum()/car.shape[0]*100

car = car.dropna(subset=['year', 'odometer', 'manufacturer', 'model'])

car.duplicated().sum()

car = car.drop_duplicates()

manufacturer_values = car['manufacturer'].value_counts()

car['manufacturer'] = car['manufacturer'].apply(lambda x: x if str(x) in manufacturer_values[:20] else x)

region_values = car['region'].value_counts()
car['region'] = car['region'].apply(lambda x: x if str(x) in region_values[:50] else x)

model_values = car['model'].value_counts()
car['model'] = car['model'].apply(lambda x: x if str(x) in model_values[:50] else x)

price_percentile25 = car['price'].quantile(0.25)
price_percentile75 = car['price'].quantile(0.75)
price_iqr = price_percentile75 - price_percentile25
price_upper_limit = price_percentile75 + 1.5 * price_iqr
price_lower_limit = car['price'].quantile(0.15)
car = car[(car['price'] < price_upper_limit) & (car['price'] > price_lower_limit)]

odometer_percentile75 = car['odometer'].quantile(0.75)
odometer_percentile25 = car['odometer'].quantile(0.25)
odometer_iqr = odometer_percentile75 - odometer_percentile25
odometer_upper_limit = odometer_percentile75 + 1.5 * odometer_iqr
odometer_lower_limit = car['odometer'].quantile(0.05)
car = car[(car['odometer'] < odometer_upper_limit) & (car['odometer'] > odometer_lower_limit)]

car['odometer'] = car['odometer'].astype(int)
car['year'] = car['year'].astype(int)

car = car[car['year'] > 1996]
car.shape

car['car_age'] = 2022 - car['year']
car.drop(['year'], axis = 1, inplace = True)
car.describe()

# finding unique list of manufacturer. There are NULL values.
car['manufacturer'].unique()

model_car = car.loc[car['manufacturer'].isnull(), ['model']]

model_car['model'].unique()[:50]

model_car.shape

car.shape

# update 'manufacturer' to 'other' when its NULL
car.loc[car['manufacturer'].isnull(), ['manufacturer']] = 'other'

car[car['region'].isnull() == True]

car['region'].unique()[:10]

# removing all records which has price = 0.
car = car[car['price'] != 0]

car['car_age'].unique()

# 'model' column has lot of inconsistent data eg). Anything, sequoia limited, 30 YEARS.EXP.
car['model'].unique()

len(car)

# Identify the no.of missing values in each column and their percentage compared to total.
missing_vals = car.isnull().sum().sort_values(ascending = False)
(missing_vals/len(car))*100

# Removing rows which has less than 5% of NULLs in columns.
car=car.dropna(subset=['model','fuel','transmission','title_status'])

car.shape

car['cylinders'].unique()

car['type'].unique()

# Using forward fill for the columns - 'paint_color', 'drive', 'cylinders', 'type'
car['type'] = car['type'].ffill()
car['paint_color'] = car['paint_color'].ffill()
car['drive'] = car['drive'].bfill()
car['cylinders'] = car['cylinders'].ffill()

car.isnull().sum()

car['condition'].unique()

# updating the condition as 'new' for all vehicles whose year is 2019 and above
#car.loc[car.year>=2019, 'condition'] = car.loc[car.year>=2019, 'condition'].fillna('new')

car.groupby(['condition']).count()['car_age']

# Find the total distinct values for 'condition'
conditions = list(car['condition'].unique())
conditions.pop(3) # removing null value from list
conditions

# Find the corresponding mean value of 'odometer' for each value in 'condition'
mean_odometer_per_condition = car.groupby('condition')['odometer'].mean().reset_index()
mean_odometer_per_condition

excellent_odo_mean = car[car['condition'] == 'excellent']['odometer'].mean()
good_odo_mean = car[car['condition'] == 'good']['odometer'].mean()
like_new_odo_mean = car[car['condition'] == 'like new']['odometer'].mean()
salvage_odo_mean = car[car['condition'] == 'salvage']['odometer'].mean()
fair_odo_mean = car[car['condition'] == 'fair']['odometer'].mean()

print('Like new average odometer:', round( like_new_odo_mean,2))
print('Excellent average odometer:', round( excellent_odo_mean,2))
print('Good average odometer:', round( good_odo_mean,2))
print('Fair average odometer:', round( fair_odo_mean,2))
print('Salvage average odometer:', round( salvage_odo_mean,2))

# Update the 'condition' based on the average 'odometer' values for each 'condition'

car.loc[car['odometer'] <= like_new_odo_mean, 'condition'] = car.loc[car['odometer'] <= like_new_odo_mean, 'condition'].fillna('like new')

car.loc[car['odometer'] >= fair_odo_mean, 'condition'] = car.loc[car['odometer'] >= fair_odo_mean, 'condition'].fillna('fair')

car.loc[((car['odometer'] > good_odo_mean) &
       (car['odometer'] <= excellent_odo_mean)), 'condition'] = car.loc[((car['odometer'] > good_odo_mean) &
       (car['odometer'] <= excellent_odo_mean)), 'condition'].fillna('excellent')

car.loc[((car['odometer'] > like_new_odo_mean) &
       (car['odometer'] <= good_odo_mean)), 'condition'] = car.loc[((car['odometer'] > like_new_odo_mean) &
       (car['odometer'] <= good_odo_mean)), 'condition'].fillna('good')

car.loc[((car['odometer'] > good_odo_mean) &
       (car['odometer'] <= fair_odo_mean)), 'condition'] = car.loc[((car['odometer'] > good_odo_mean) &
       (car['odometer'] <= fair_odo_mean)), 'condition'].fillna('salvage')

# Removing region since we already have 'county'
car=car.drop('region', axis=1)

# Removing rows which has NULLs in conditon and odometer.
car=car.dropna(subset=['odometer','condition'])

# Identify the no.of missing values in each column and their percentage compared to total.
missing_vals = car.isnull().sum().sort_values(ascending = False)
(missing_vals/len(car))*100

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.compose import ColumnTransformer

pipe_categorical = Pipeline(
    steps = [('ordinal_encoder', OrdinalEncoder(categories = [[ 'salvage', 'fair', 'unknown', 'good', 'excellent', 'like new', 'new']])),
             ('one_hot_encoder', OneHotEncoder(sparse = False, drop = 'first'))]
)

pipe_numerical = Pipeline(
    steps = [('standard_scaler', StandardScaler())]
)

column_transformer = ColumnTransformer(transformers = [
    ('condition_pipe_trans', pipe_categorical['ordinal_encoder'], ['condition']),
    ('categorical_pipe_trans', pipe_categorical['one_hot_encoder'], ['model', 'manufacturer', 'fuel', 'cylinders','title_status', 'transmission', 'drive', 'type', 'paint_color']),
    ('numerical_pipe_trans', pipe_numerical, ['odometer'])
])

lc = LabelEncoder()

# List of columns to encode
columns_to_encode = ['model','manufacturer' , 'fuel', 'cylinders','title_status', 'transmission', 'drive', 'type', 'paint_color', 'condition']

# Iterate over each column and encode its values
for col in columns_to_encode:
    car[col] = lc.fit_transform(car[col])

car.dtypes

X = car.drop(['price'],axis=1)
y = car['price']

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42)

# Convert the data to float32 type
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

# perform the scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64,activation='relu'),
    tf.keras.layers.Dense(64,activation='relu'),
    tf.keras.layers.Dense(1, activation='linear')
])

model.compile(loss = tf.keras.losses.mean_absolute_error, optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.02), metrics=['mae'])

model.fit(X_train, y_train,
          epochs=200, batch_size=16, verbose=0)
test_mse_score, test_mae_score = model.evaluate(X_test, y_test)


print("test_mae_score: ", test_mae_score)

print("test_mse_score: ", test_mse_score)

y_pred = model.predict(X_test)
y_pred[0]
print("y_pred[0]: ", y_pred[0])

r2_accuracy = metrics.r2_score(y_test, y_pred)
print("Accuracy : ", r2_accuracy)